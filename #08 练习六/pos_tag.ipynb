{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于隐马尔科夫模型的词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PKU98 = \"pku98\"\n",
    "PKU199801 = os.path.join(PKU98, '199801.txt')\n",
    "PKU199801_TRAIN = os.path.join(PKU98, '199801-train.txt')\n",
    "PKU199801_TEST = os.path.join(PKU98, '199801-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r, u, n, v, v, v\n",
      "他/r 的/u 希望/n 是/v 希望/v 上学/v\n",
      "他/代词 的/助词 希望/名词 是/动词 希望/动词 上学/动词\n"
     ]
    }
   ],
   "source": [
    "from  pyhanlp import *\n",
    "\n",
    "HMMPOSTagger = JClass('com.hankcs.hanlp.model.hmm.HMMPOSTagger')\n",
    "AbstractLexicalAnalyzer = JClass('com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer')\n",
    "PerceptronSegmenter = JClass('com.hankcs.hanlp.model.perceptron.PerceptronSegmenter')\n",
    "CRFSegmenter = JClass('com.hankcs.hanlp.model.crf.CRFSegmenter')\n",
    "FirstOrderHiddenMarkovModel = JClass('com.hankcs.hanlp.model.hmm.FirstOrderHiddenMarkovModel')\n",
    "\n",
    "def train_hmm_pos(corpus, model):\n",
    "    tagger = HMMPOSTagger(model)  # 创建词性标注器\n",
    "    tagger.train(corpus)  # 训练\n",
    "    return tagger\n",
    "\n",
    "# 输入分词结果，预测词性\n",
    "hmm_tagger = train_hmm_pos(PKU199801_TRAIN, FirstOrderHiddenMarkovModel())\n",
    "print(', '.join(hmm_tagger.tag(\"他\", \"的\", \"希望\", \"是\", \"希望\", \"上学\")))  # 预测\n",
    "\n",
    "# 同时进行分词和词性标注\n",
    "analyzer = AbstractLexicalAnalyzer(CRFSegmenter(), hmm_tagger)  # 构造词法分析器\n",
    "print(analyzer.analyze(\"他的希望是希望上学\"))  # 分词+词性标注\n",
    "\n",
    "# 翻译词性代码\n",
    "print(analyzer.analyze(\"他的希望是希望上学\").translateLabels())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于条件随机场的词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MODEL = os.path.join(PKU98, 'pos.bin')\n",
    "\n",
    "CRFPOSTagger = JClass('com.hankcs.hanlp.model.crf.CRFPOSTagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crf_pos(corpus):\n",
    "    tagger = CRFPOSTagger(None)  # 创建空白标注器\n",
    "    tagger.train(corpus, POS_MODEL)  # 训练\n",
    "    return tagger\n",
    "\n",
    "# 训练模型比较耗时，训练以后可以暂时注释掉\n",
    "#tagger = train_crf_pos(PKU199801_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r, u, n, v, v, v\n",
      "李狗蛋/nr 的/u 希望/n 是/v 希望/v 上学/v\n"
     ]
    }
   ],
   "source": [
    "crf_tagger = CRFPOSTagger(POS_MODEL) # 加载\n",
    "print(', '.join(crf_tagger.tag(\"他\", \"的\", \"希望\", \"是\", \"希望\", \"上学\")))  # 预测\n",
    "analyzer = AbstractLexicalAnalyzer(CRFSegmenter(), crf_tagger)  # 构造词法分析器\n",
    "print(analyzer.analyze(\"李狗蛋的希望是希望上学\"))  # 分词+词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准确率测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM\t44.99%: \n",
      "CRF\t82.17%: \n"
     ]
    }
   ],
   "source": [
    "PosTagUtil = JClass('com.hankcs.hanlp.dependency.nnparser.util.PosTagUtil')\n",
    "\n",
    "print(\"HMM\\t%.2f%%: \" % (PosTagUtil.evaluate(hmm_tagger, PKU199801_TEST)))\n",
    "print(\"CRF\\t%.2f%%: \" % (PosTagUtil.evaluate(crf_tagger, PKU199801_TEST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结巴词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思想：将词性标注看成序列标注问题，与分词同时进行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入概率矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个状态都是分词标签和词性标签的组合，例如('E', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 概率值都是取对数之后的结果\n",
    "from prob_start import P as start_p # 状态初始概率\n",
    "from prob_trans import P as trans_p # 状态转移概率\n",
    "from prob_emit import P as emit_p   # 状态发射概率\n",
    "from char_state_tab import P as states # 每个字的状态\n",
    "\n",
    "MIN_FLOAT = -3.14e100\n",
    "MIN_INF = float(\"-inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi算法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于隐状态数量巨大，实现的过程中要考虑从一个时刻向另一个时刻转移时尽量筛选出可能的状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]  # tabular\n",
    "    mem_path = [{}]\n",
    "    all_states = trans_p.keys()\n",
    "    for y in states.get(obs[0], all_states):  # 遍历第一个字可能的隐状态\n",
    "        V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT)\n",
    "        mem_path[0][y] = ''\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        mem_path.append({})\n",
    "        # 获取前一个时刻可能的状态\n",
    "        prev_states = [x for x in mem_path[t - 1].keys() if len(trans_p[x]) > 0]\n",
    "        \n",
    "        # 获取前一时刻的状态可以转移到的所有状态\n",
    "        prev_states_expect_next = set((y for x in prev_states for y in trans_p[x].keys()))\n",
    "        \n",
    "        # 将利用前一时刻的状态计算出的当前时刻的状态，与当前字可能的状态求交集，进一步对可能的状态进行过滤\n",
    "        obs_states = set(states.get(obs[t], all_states)) & prev_states_expect_next\n",
    "\n",
    "        if not obs_states:\n",
    "            obs_states = prev_states_expect_next if prev_states_expect_next else all_states\n",
    "\n",
    "        for y in obs_states:\n",
    "            prob, state = max((V[t - 1][y0] + trans_p[y0].get(y, MIN_INF) +\n",
    "                               emit_p[y].get(obs[t], MIN_FLOAT), y0) for y0 in prev_states)\n",
    "            V[t][y] = prob\n",
    "            mem_path[t][y] = state\n",
    "\n",
    "    last = [(V[-1][y], y) for y in mem_path[-1].keys()]\n",
    "    # if len(last)==0:\n",
    "    #     print obs\n",
    "    prob, state = max(last)\n",
    "\n",
    "    route = [None] * len(obs)\n",
    "    i = len(obs) - 1\n",
    "    while i >= 0:\n",
    "        route[i] = state\n",
    "        state = mem_path[i][state]\n",
    "        i -= 1\n",
    "    return (prob, route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __cut(sentence, char_state_tab_P, start_P, trans_P, emit_P):\n",
    "    prob, pos_list = viterbi(\n",
    "        sentence, char_state_tab_P, start_P, trans_P, emit_P)\n",
    "    begin, nexti = 0, 0\n",
    "\n",
    "    for i, char in enumerate(sentence):\n",
    "        pos = pos_list[i][0]\n",
    "        if pos == 'B':\n",
    "            begin = i\n",
    "        elif pos == 'E':\n",
    "            yield (sentence[begin:i + 1], pos_list[i][1])\n",
    "            nexti = i + 1\n",
    "        elif pos == 'S':\n",
    "            yield (char, pos_list[i][1])\n",
    "            nexti = i + 1\n",
    "    if nexti < len(sentence):\n",
    "        yield (sentence[nexti:], pos_list[nexti][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('扬帆', 'nz'),\n",
       " ('远东', 'n'),\n",
       " ('做', 'v'),\n",
       " ('与', 'p'),\n",
       " ('中国', 'ns'),\n",
       " ('合作', 'vn'),\n",
       " ('的', 'uj'),\n",
       " ('先行', 'n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"扬帆远东做与中国合作的先行\"\n",
    "list(__cut(sentence,  states, start_p, trans_p, emit_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
