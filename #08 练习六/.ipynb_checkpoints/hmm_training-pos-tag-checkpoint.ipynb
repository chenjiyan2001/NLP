{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HMM模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用MSR语料训练HMM模型的初始概率向量，状态转移概率矩阵和发射概率矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PKU98 = \"../练习六/pku98\"\n",
    "PKU199801 = os.path.join(PKU98, '199801.txt')\n",
    "PKU199801_TRAIN = os.path.join(PKU98, '199801-train.txt')\n",
    "PKU199801_TEST = os.path.join(PKU98, '199801-test.txt')\n",
    "\n",
    "MIN_FLOAT = -3.14e100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../练习六/pku98\\\\199801-train.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PKU199801_TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换语料格式\n",
    "\n",
    "将语料中的每个句子的分词结果进行转换，每句话放到一个元组中，第一个元素是每个字符构成的列表，第二个元素是每个字符的分词标签。所有的句子放到一个列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "fw = open(\"output.txt\",\"w\",encoding=\"utf-8\")\n",
    "with open(PKU199801_TRAIN,encoding=\"utf-8\") as fr:\n",
    "    for line in fr:\n",
    "        line = re.sub(r\"\\[\",\"\",line)\n",
    "        line = re.sub(r\"\\]/[a-z]\",\"\",line)\n",
    "        fw.write(line)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_tagging(input_file):\n",
    "    input_data = open(input_file, 'r', encoding='utf-8')\n",
    "    ret_list = []\n",
    "    for line in input_data.readlines():\n",
    "        char_list = []\n",
    "        tag_list = []\n",
    "        word_list = line.strip().split()\n",
    "        if len(word_list) == 0:\n",
    "            continue\n",
    "        for word in word_list:\n",
    "            \n",
    "            word,pos = word.split(\"/\")\n",
    "            print\n",
    "            if len(word) == 1:\n",
    "                char_list.append(word)\n",
    "                tag_list.append((\"S\",pos))\n",
    "            elif len(word) >= 2:\n",
    "                char_list.append(word[0])\n",
    "                tag_list.append((\"B\",pos))\n",
    "                for w in word[1: len(word)-1]:\n",
    "                    char_list.append(w)\n",
    "                    tag_list.append((\"M\",pos))\n",
    "                tag_list.append((\"E\",pos))\n",
    "                char_list.append(word[-1])\n",
    "                assert(len(char_list)==len(tag_list))\n",
    "        ret_list.append((char_list,tag_list))\n",
    "    input_data.close()\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迈向/v\n",
      "充满/v\n",
      "希望/n\n",
      "的/u\n",
      "新/a\n",
      "世纪/n\n",
      "——/w\n",
      "一九九八年/t\n",
      "新年/t\n",
      "讲话/n\n",
      "（/w\n",
      "附/v\n",
      "图片/n\n",
      "１/m\n",
      "张/q\n",
      "）/w\n",
      "中共中央/nt\n",
      "总书记/n\n",
      "、/w\n",
      "国家/n\n",
      "主席/n\n",
      "江泽民/nr\n",
      "（/w\n",
      "一九九七年/t\n",
      "十二月/t\n",
      "三十一日/t\n",
      "）/w\n",
      "１２月/t\n",
      "３１日/t\n",
      "，/w\n",
      "中共中央/nt\n",
      "总书记/n\n",
      "、/w\n",
      "国家/n\n",
      "主席/n\n",
      "江泽民/nr\n",
      "发表/v\n",
      "１９９８年/t\n",
      "新年/t\n",
      "讲话/n\n",
      "《/w\n",
      "迈向/v\n",
      "充满/v\n",
      "希望/n\n",
      "的/u\n",
      "新/a\n",
      "世纪/n\n",
      "》/w\n",
      "。/w\n",
      "（/w\n",
      "新华社/nt\n",
      "记者/n\n",
      "兰红光/nr\n",
      "摄/Vg\n",
      "）/w\n",
      "同胞/n\n",
      "们/k\n",
      "、/w\n",
      "朋友/n\n",
      "们/k\n",
      "、/w\n",
      "女士/n\n",
      "们/k\n",
      "、/w\n",
      "先生/n\n",
      "们/k\n",
      "：/w\n",
      "在/p\n",
      "１９９８年/t\n",
      "来临/v\n",
      "之际/f\n",
      "，/w\n",
      "我/r\n",
      "十分/m\n",
      "高兴/a\n",
      "地/u\n",
      "通过/p\n",
      "[中央/n\n",
      "人民/n\n",
      "广播/vn\n",
      "电台/n]/nt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2d717aad117c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mret_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcharacter_tagging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPKU199801_TRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f3c7e11dd01c>\u001b[0m in \u001b[0;36mcharacter_tagging\u001b[1;34m(input_file)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "ret_list = character_tagging(output.tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ret_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1dd44c4489b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ret_list' is not defined"
     ]
    }
   ],
   "source": [
    "print(ret_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计频率\n",
    "\n",
    "统计初始状态发生的频率，状态转移频率和发射频率，分别放到三个字典中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_dict = {\"B\":0,\"M\":0,\"E\":0,\"S\":0}\n",
    "transition_dict = {}\n",
    "emission_dict = {}\n",
    "\n",
    "char_list = []\n",
    "for item in ret_list:\n",
    "    char_list.extend(item[0])\n",
    "char_set = set(char_list)\n",
    "\n",
    "for item in ret_list:\n",
    "    try:\n",
    "        # 计算初始状态频率\n",
    "        first_state = item[1][0]\n",
    "        initial_state_dict[first_state] += 1\n",
    "        \n",
    "        # 计算状态转移频率\n",
    "        for i in range(len(item[1])-1):\n",
    "            left_state = item[1][i]\n",
    "            if left_state not in transition_dict:\n",
    "                transition_dict[left_state] = {\"B\":0,\"M\":0,\"E\":0,\"S\":0}\n",
    "            right_state = item[1][i+1]\n",
    "            transition_dict[left_state][right_state] += 1\n",
    "        \n",
    "        # 计算发射频率\n",
    "        for i in range(len(item[1])):\n",
    "            state = item[1][i]\n",
    "            if state not in emission_dict:\n",
    "                emission_dict[state] = {}\n",
    "            char = item[0][i]\n",
    "            if char not in emission_dict[state]:\n",
    "                emission_dict[state][char] = 0\n",
    "            emission_dict[state][char] += 1      \n",
    "    except:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 60460, 'M': 0, 'E': 0, 'S': 26458}\n"
     ]
    }
   ],
   "source": [
    "print(initial_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': {'B': 0, 'E': 1039906, 'M': 215149, 'S': 0},\n",
      " 'E': {'B': 594480, 'E': 0, 'M': 0, 'S': 659674},\n",
      " 'M': {'B': 0, 'E': 215149, 'M': 211874, 'S': 0},\n",
      " 'S': {'B': 600115, 'E': 0, 'M': 0, 'S': 427204}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(transition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算概率\n",
    "\n",
    "注意这里计算概率值的同时取了对数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算初始状态概率向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "prob_start = {}\n",
    "for key in initial_state_dict:\n",
    "    total_freq = sum(initial_state_dict.values())\n",
    "    if initial_state_dict[key] != 0:\n",
    "        prob_start[key] = math.log(initial_state_dict[key]/total_freq)\n",
    "    else:\n",
    "        prob_start[key] = MIN_FLOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': -0.3629831561081316,\n",
       " 'M': -3.14e+100,\n",
       " 'E': -3.14e+100,\n",
       " 'S': -1.1894065754192553}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算状态转移概率矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_trans = {}\n",
    "for key in transition_dict:\n",
    "    total_freq = sum(transition_dict[key].values())\n",
    "    prob_trans[key] = {}\n",
    "    for state in transition_dict[key]:\n",
    "        if transition_dict[key][state] != 0:\n",
    "            prob_trans[key][state] = math.log(transition_dict[key][state]/total_freq)\n",
    "        else:\n",
    "            prob_trans[key][state] = MIN_FLOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': {'B': -3.14e+100,\n",
      "       'E': -0.18804907187170708,\n",
      "       'M': -1.763603863953054,\n",
      "       'S': -3.14e+100},\n",
      " 'E': {'B': -0.7465294468210316,\n",
      "       'E': -3.14e+100,\n",
      "       'M': -3.14e+100,\n",
      "       'S': -0.6424707470719607},\n",
      " 'M': {'B': -3.14e+100,\n",
      "       'E': -0.6855070645928693,\n",
      "       'M': -0.7008461175870558,\n",
      "       'S': -3.14e+100},\n",
      " 'S': {'B': -0.5375864716182832,\n",
      "       'E': -3.14e+100,\n",
      "       'M': -3.14e+100,\n",
      "       'S': -0.8774461242373575}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(prob_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算发射概率矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_emit = {}\n",
    "for key in emission_dict:\n",
    "    total_freq = sum(emission_dict[key].values())\n",
    "    prob_emit[key] = {}\n",
    "    for char in emission_dict[key]:\n",
    "        if emission_dict[key][char] != 0:\n",
    "            prob_emit[key][char] = math.log(emission_dict[key][char]/total_freq)\n",
    "        else:\n",
    "            prob_emit[key][char] = MIN_FLOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.295103427459627"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_emit[\"B\"]['好']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM分词模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['扬帆', '远东', '做', '与', '中国', '合作', '的', '先行']\n"
     ]
    }
   ],
   "source": [
    "PrevStatus = {\n",
    "    'B': 'ES',\n",
    "    'M': 'MB',\n",
    "    'S': 'SE',\n",
    "    'E': 'BM'\n",
    "}\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]  # tabular\n",
    "    path = {}\n",
    "    for y in states:  # init\n",
    "        V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT)\n",
    "        path[y] = [y]\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "        for y in states:\n",
    "            em_p = emit_p[y].get(obs[t], MIN_FLOAT)\n",
    "            (prob, state) = max(\n",
    "                [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])\n",
    "            V[t][y] = prob\n",
    "            newpath[y] = path[state] + [y]\n",
    "        path = newpath\n",
    "\n",
    "    (prob, state) = max((V[len(obs) - 1][y], y) for y in 'ES')\n",
    "\n",
    "    return (prob, path[state])\n",
    "\n",
    "def __cut(sentence, prob_start, prob_trans, prob_emit):\n",
    "    prob, pos_list = viterbi(sentence, 'BMES', prob_start, prob_trans, prob_emit)\n",
    "    begin, nexti = 0, 0\n",
    "    # print pos_list, sentence\n",
    "    for i, char in enumerate(sentence):\n",
    "        pos = pos_list[i]\n",
    "        if pos == 'B':\n",
    "            begin = i\n",
    "        elif pos == 'E':\n",
    "            yield sentence[begin:i + 1]\n",
    "            nexti = i + 1\n",
    "        elif pos == 'S':\n",
    "            yield char\n",
    "            nexti = i + 1\n",
    "    if nexti < len(sentence):\n",
    "        yield sentence[nexti:]\n",
    "        \n",
    "sentence = \"扬帆远东做与中国合作的先行\"\n",
    "print(list(__cut(sentence,prob_start, prob_trans, prob_emit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准确率测试\n",
    "\n",
    "分别在msr_gold和msr_test上对训练的模型进行测试，发现结果略有不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:78.03 R:80.05 F1:79.03 OOV-R:36.76 IV-R:81.23\n"
     ]
    }
   ],
   "source": [
    "from common import *\n",
    "word_dict = load_dictionary(msr_dict)\n",
    "\n",
    "with open(msr_gold,encoding=\"utf-8\") as test, open(msr_output, 'w', encoding=\"utf-8\") as output:\n",
    "    for line in test:\n",
    "        output.write(\"  \".join(__cut(re.sub(\"\\\\s+\", \"\", line),prob_start, prob_trans, prob_emit)))\n",
    "        output.write(\"\\n\")\n",
    "        \n",
    "print(\"P:%.2f R:%.2f F1:%.2f OOV-R:%.2f IV-R:%.2f\" % prf(msr_gold, msr_output, word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:77.85 R:79.92 F1:78.87 OOV-R:36.73 IV-R:81.10\n"
     ]
    }
   ],
   "source": [
    "with open(msr_test,encoding=\"utf-8\") as test, open(msr_output, 'w', encoding=\"utf-8\") as output:\n",
    "    for line in test:\n",
    "        output.write(\"  \".join(__cut(line.strip(),prob_start, prob_trans, prob_emit)))\n",
    "        output.write(\"\\n\")\n",
    "        \n",
    "print(\"P:%.2f R:%.2f F1:%.2f OOV-R:%.2f IV-R:%.2f\" % prf(msr_gold, msr_output, word_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析造成不同结果的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现上面用msr_gold语料和msr_test语料的评测结果略有不同，我们把两个语料中不同的句子进行输出。发现造成不同的原因是多一个或少一个引号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在对“东方红三号”卫星的测控过程中，西安卫星测控中心首次采用同国际标准兼容的新型测控网，对卫星成功地实施了３次变轨和多次定点捕获进行轨道修正。“\n",
      "在对“东方红三号”卫星的测控过程中，西安卫星测控中心首次采用同国际标准兼容的新型测控网，对卫星成功地实施了３次变轨和多次定点捕获进行轨道修正。\n",
      "\n",
      "远望号”航天远洋测量船实现了从海上测量到测控的新跨越。\n",
      "“远望号”航天远洋测量船实现了从海上测量到测控的新跨越。\n",
      "\n",
      "去年以来，这个工段各个班组的日核算从未间断过，经济效益与日俱增。“\n",
      "去年以来，这个工段各个班组的日核算从未间断过，经济效益与日俱增。\n",
      "\n",
      "整天算帐，烦不烦？”\n",
      "“整天算帐，烦不烦？”\n",
      "\n",
      "记者问正在现场忙碌的工人。“\n",
      "记者问正在现场忙碌的工人。\n",
      "\n",
      "烦也得算！”\n",
      "“烦也得算！”\n",
      "\n",
      "建材公司电焊工苗磊，参加工作不足６年，先后参加过５７座大罐的施工，各种焊口达两万多道，焊缝１万多延长米，合格率达１００％，优良率达９５％以上。“\n",
      "建材公司电焊工苗磊，参加工作不足６年，先后参加过５７座大罐的施工，各种焊口达两万多道，焊缝１万多延长米，合格率达１００％，优良率达９５％以上。\n",
      "\n",
      "说主人话，干主人活，尽主人责”，已成为百里油田的一道风景线。\n",
      "“说主人话，干主人活，尽主人责”，已成为百里油田的一道风景线。\n",
      "\n",
      "农安、榆树、公主岭、梨树等产粮大县（市），发挥粮多优势，采取得力措施建设生产、防疫、加工、销售四位一体的“生猪工程”、“肉牛工程”，取得了长足进展。\n",
      "农安、榆树、公主岭、梨树等产粮大县（市），发挥粮多优势，采取得力措施建设生产、防疫、加工、销售四位 一体的“生猪工程”、“肉牛工程”，取得了长足进展。\n",
      "\n",
      "在演马庄矿百米井下工作面，记者见到了正在挥锨装煤的矿党委书记张天福和矿长杨西平。“\n",
      "在演马庄矿百米井下工作面，记者见到了正在挥锨装煤的矿党委书记张天福和矿长杨西平。\n",
      "\n",
      "工人三班倒，班班见领导”，这是去年以来全局开始形成的制度，７个矿６０多名矿领导每月都坚持下井１５个班以上。\n",
      "“工人三班倒，班班见领导”，这是去年以来全局开始形成的制度，７个矿６０多名矿领导每月都坚持下井１５个班以上。\n",
      "\n",
      "---河区人堵水道，水才冲人路。“\n",
      "---河区人堵水道，水才冲人路。\n",
      "\n",
      "岁岁防洪不见洪”麻痹了人们的防洪意识，新建的企业和房屋把河道侵占的越来越窄，造成阻水分流，加重了灾情。\n",
      "“岁岁防洪不见洪”麻痹了人们的防洪意识，新建的企业和房屋把河道侵占的越来越窄，造成阻水分流，加重了灾情。\n",
      "\n",
      "新华社北京７月１３日电（中央人民广播电台记者刘振英、新华社记者李安定）由国务院召开的全国粮食购销工作会议７月９日至１１日在北京举行。\n",
      "新华社北京７月１３日电（中央人民广播电台记者刘振英、新华社记者李安定）由国务院召开的全国粮食购销工作会议 ７月９日至１１日在北京举行。\n",
      "\n",
      "从１９７８年到１９９６年，在改革开放的大潮中，秦家山村经过１８年的艰苦拼搏，总产值由２０万元递增到３.４亿元，人均纯收入由１００来元提高到７０００元，一跃成为村庄城镇化、管理企业化的富裕文明的“小康村”、“亿元村”，成为社会主义新农村。\n",
      "从 １９７８年到１９９６年，在改革开放的大潮中，秦家山村经过１８年的艰苦拼搏，总产值由２０万元递增到３.４亿元，人均纯收入由１００来元提高到７０００元，一跃成为村庄城镇化、管理企业化的富裕文明的“小康村”、“亿元村”，成为社会主义新农村。\n",
      "\n",
      "与此相反，全省小氮肥企业将由５７家减至４５家，磷肥企业由４４家减至３５家，４０家橡胶厂也将减少一半。“\n",
      "与此相反，全省小氮肥企业将由５７家减至４５家，磷肥企业由４４家减至３５家，４０家橡胶厂也将减少一半。\n",
      "\n",
      "八五”以来，全省化工行业实现利税和利润年均递增１１.５９％和１５.８％。\n",
      "“八五”以来，全省化工行业实现利税和利润年均递增１１.５９％和１５.８％。\n",
      "\n",
      "目前，“彩虹”彩管已占国内市场的１／３，国产化率达到８０％以上。“\n",
      "目前，“彩虹”彩管已占国内市场的１／３，国产化率达到８０％以上。\n",
      "\n",
      "长虹”、“熊猫”彩电，“海尔”冰箱，“小天鹅”洗衣机，“春兰”空调等国产名牌产品，也都积极引进消化国外先进技术，为己所用，增强了竞争实力。\n",
      "“长虹”、“熊猫”彩电，“海尔”冰箱，“小天鹅”洗衣机，“春兰”空调等国产名牌产品，也都积极引进消化国外先进技术，为己所用，增强了竞争实力。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(msr_gold,encoding=\"utf-8\") as gold,open(msr_test,encoding=\"utf-8\") as test:\n",
    "    gold_lines = gold.readlines()\n",
    "    test_lines = test.readlines()\n",
    "    for i in range(len(gold_lines)):\n",
    "        if re.sub(\"\\\\s+\", \"\", gold_lines[i]) != test_lines[i].strip():\n",
    "            print(re.sub(\"\\\\s+\", \"\", gold_lines[i]))\n",
    "            print(test_lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词速度评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10018.64 万字/秒\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10018.640869461364"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "def evaluate_speed(text):\n",
    "    start_time = time.time()\n",
    "    for i in range(pressure):\n",
    "        __cut(text,prob_start, prob_trans, prob_emit)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    seg_speed = len(text) * pressure / 10000 / elapsed_time\n",
    "    print('%.2f 万字/秒' % (seg_speed))\n",
    "    return seg_speed\n",
    "\n",
    "text = \"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"\n",
    "pressure = 10000\n",
    "evaluate_speed(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3676.03 万字/秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "total_text_length = 0\n",
    "with open(msr_test,encoding=\"utf-8\") as test:\n",
    "    for line in test:\n",
    "        total_text_length += len(line.strip())\n",
    "        __cut(line.strip(),prob_start, prob_trans, prob_emit)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "seg_speed = total_text_length / 10000 / elapsed_time\n",
    "\n",
    "print('%.2f 万字/秒' % (seg_speed))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
