{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统特征构建方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "重点：特征构建\n",
    "\n",
    "传统方法：词袋模型，one-hot编码，TF-IDF\n",
    "\n",
    "词袋模型的缺陷：\n",
    "1. 数据稀疏\n",
    "2. 维度巨大\n",
    "3. 没有考虑上下文信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![onehot](figures/onehot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Word2vec 是 [Mikolov et al.](http://arxiv.org/pdf/1301.3781.pdf) 提出一种训练词向量的方法。思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用数学的方法来研究词与词之间的关系。比如下图我们将词汇表里的词用\"Royalty\",\"Masculinity\", \"Femininity\"和\"Age\"4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/word2vec/wordvector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了词向量，我们就可以分析词之间的关系，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：\n",
    "$$\\vec {King} - \\vec {Man} + \\vec {Woman} = \\vec {Queen}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/word2vec/goodexample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec有 Continuous Bag-of-Words model (CBOW) and the Skip-Gram model 两种训练方式，前者是用一个词序列窗口中的其他词来预测中心词，后者则是用中心词来预测其他词。在实际使用时，一般使用 Skip-Gram 结合 [Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) 进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CBOW & skip-gram](figures/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Tree（霍夫曼树）\n",
    "哈夫曼树是一种带权路径长度最短的二叉树，也称为最优二叉树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有A B C D 四个词，数字表示词频，构造过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![softmax](figures/huffman1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![softmax](figures/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在word2vec中，从隐藏层到输出的softmax,为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。\n",
    "采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(+) = \\sigma(x_w^T\\theta) = \\frac{1}{1+e^{-x_w^T\\theta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$x_w$是当前内部节点的词向量，而θ则是我们需要从训练样本求出的逻辑回归的模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![softmax](figures/CBOW.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![softmax](figures/hierarchicalsoftmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用最大似然法来寻找所有节点的词向量和所有内部节点$\\theta$。以上面的$w_2$例子来看，我们期望最大化下面的似然函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\prod_{i=1}^3P(n(w_i),i) = (1- \\frac{1}{1+e^{-x_w^T\\theta_1}})(1- \\frac{1}{1+e^{-x_w^T\\theta_2}})\\frac{1}{1+e^{-x_w^T\\theta_3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。实际计算时为了减少梯度计算量，并没有把所有样本的似然乘起来得到真正的训练集最大似然，每次只用一个样本更新梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:20:01.850814Z",
     "start_time": "2021-06-02T02:20:01.197709Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# gensim 库中包含了 word2vec 模块\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T01:32:44.537231Z",
     "start_time": "2021-06-02T01:32:42.504810Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence('./in_the_name_of_people_segment.txt')\n",
    "model = word2vec.Word2Vec(sentences, hs=1,min_count=1,window=3,vector_size=200) # hs:是否使用Hierarchical Softmax\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "fname = \"word2vec.model\"\n",
    "model = word2vec.Word2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('沙瑞金', 0.9531140327453613),\n",
       " ('侯亮平', 0.9398585557937622),\n",
       " ('李达康', 0.9386978149414062),\n",
       " ('祁同伟', 0.929216742515564),\n",
       " ('陆亦可', 0.9156097173690796),\n",
       " ('季昌明', 0.9148707985877991),\n",
       " ('开玩笑', 0.9083293676376343),\n",
       " ('咱', 0.9017901420593262),\n",
       " ('叹', 0.894492506980896),\n",
       " ('牙疼', 0.8940712213516235)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取相似词\n",
    "word = \"高育良\"\n",
    "model.wv.similar_by_word(word, topn =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01535346,  0.3604732 ,  0.03981665,  0.18880066,  0.6338018 ,\n",
       "        0.28114927,  0.05964074,  0.25281897, -0.4284452 ,  0.26713574,\n",
       "        0.03453567, -0.02314873, -0.11728395,  0.0027645 ,  0.03520486,\n",
       "       -0.20720787,  0.03071065,  0.42624998, -0.5083926 , -0.7377485 ,\n",
       "        0.5243193 ,  0.20968246, -0.28458962, -0.07346202, -0.08507089,\n",
       "       -0.21286307, -0.07201812,  0.23646404, -0.6476278 ,  0.29151472,\n",
       "        0.35351804, -0.35985184, -0.44982687, -0.01178524, -0.19293083,\n",
       "        0.16130134,  0.36793253,  0.10603456,  0.16723225, -0.50945866,\n",
       "       -0.2526467 ,  0.09486602, -0.01863177,  0.12769422,  0.36140692,\n",
       "        0.22467852, -0.00612385,  0.10622723,  0.10884923,  0.36551276,\n",
       "        0.4004716 ,  0.17523465, -0.06098179, -0.3204234 , -0.04869512,\n",
       "       -0.11654644, -0.07157538, -0.5358171 , -0.4994673 ,  0.01718854,\n",
       "        0.06543393,  0.10025836, -0.01308374, -0.21079598, -0.3025868 ,\n",
       "        0.22448917,  0.53149885,  0.2600528 , -0.17486493,  0.31040847,\n",
       "        0.36233258,  0.5326517 ,  0.381724  , -0.48822078,  0.23961477,\n",
       "        0.23246993,  0.46601427, -0.04509705, -0.24836063, -0.19554691,\n",
       "       -0.48325804, -0.08969352, -0.406579  ,  0.05759335, -0.14096574,\n",
       "        0.23664652, -0.04043729, -0.20376422,  0.50148875,  0.02519812,\n",
       "       -0.08841907,  0.4340756 ,  0.38068566, -0.01376389,  0.5484074 ,\n",
       "        0.14804806, -0.04022859, -0.25258705, -0.17054373,  0.16111174,\n",
       "       -0.26403993,  0.60598046,  0.46385282,  0.0786548 ,  0.03691433,\n",
       "       -0.6176948 , -0.08992212, -0.0145848 , -0.3201272 ,  0.15133622,\n",
       "       -0.35908812, -0.71574485,  0.04424316, -0.2827962 ,  0.44214803,\n",
       "        0.32595748,  0.29485604, -0.7255781 , -0.5347578 , -0.68268514,\n",
       "       -0.30395606,  0.38107717,  0.24284947, -0.35231265, -0.54399985,\n",
       "        0.15268466,  0.00902909,  0.24903803, -0.01670902, -0.05826646,\n",
       "        0.15859705,  0.294344  , -0.09186728,  0.19495781,  0.01994343,\n",
       "        0.24904855,  0.34020555, -0.44191802, -0.41427228, -0.17488532,\n",
       "        0.6129411 , -0.48861626, -0.00763787, -0.12780087,  0.4567292 ,\n",
       "        0.19797128, -0.4397558 ,  0.52208364, -0.33685824, -0.26592168,\n",
       "        0.13349107, -0.24133737,  0.2311341 ,  0.47073528, -0.45746532,\n",
       "        0.34931493,  0.18228602,  0.40750304, -0.2641516 ,  0.04028269,\n",
       "        0.10635552, -0.16765858, -0.12008153, -0.20379022,  0.3125875 ,\n",
       "        0.14333665,  0.6095729 ,  0.06191113, -0.24734531, -0.3042698 ,\n",
       "       -0.2188926 ,  0.19141707,  0.33375174,  0.4086468 ,  0.3225721 ,\n",
       "        0.51505005, -0.18378349,  0.4105974 ,  0.20819902,  0.02817107,\n",
       "       -0.2771399 ,  0.11364911,  0.3367398 ,  0.03217132,  0.02727974,\n",
       "       -0.22703765,  0.22618878,  0.12716037,  0.4825638 , -0.0071381 ,\n",
       "        0.65279186, -0.02205133, -0.77234703, -0.38449514,  0.49956456,\n",
       "        0.04797019,  0.43886536, -0.8176122 , -0.08543637,  0.08333293],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取词向量\n",
    "model.wv['京州']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用Word2Vec结合传统机器学习进行情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:19:29.948448Z",
     "start_time": "2021-06-02T02:19:29.063546Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "# df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:19:40.568126Z",
     "start_time": "2021-06-02T02:19:39.653111Z"
    }
   },
   "outputs": [],
   "source": [
    "train_corpus = [line.split() for line in df['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:20:34.849475Z",
     "start_time": "2021-06-02T02:20:10.227151Z"
    }
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(\n",
    "    train_corpus,\n",
    "    vector_size=50,  # 词向量的维度\n",
    "    workers=4)  # 调用的进程数\n",
    "# 常用Word2Vec的参数：min_count,如果词频小于 min_count, word2vec 不会把这个词放入 vocab 里\n",
    "\n",
    "# 保存模型\n",
    "model.save('imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "利用训练好的词向量来进行情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:20:44.005213Z",
     "start_time": "2021-06-02T02:20:43.319779Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = word2vec.Word2Vec.load('imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:20:52.907190Z",
     "start_time": "2021-06-02T02:20:52.291893Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review'].values\n",
    "y = df['sentiment'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:21:01.409064Z",
     "start_time": "2021-06-02T02:21:01.395036Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用 word vec 的均值作为 doc vec\n",
    "def get_doc_vec(sentence, model):\n",
    "    scores = np.array([model.wv[word] for word in sentence.split() if word in model.wv])\n",
    "    return np.mean(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:21:32.053475Z",
     "start_time": "2021-06-02T02:21:09.846118Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_word2vec_train = np.array([get_doc_vec(sentence, model) for sentence in X_train])\n",
    "X_word2vec_test =  np.array([get_doc_vec(sentence, model) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:21:40.960371Z",
     "start_time": "2021-06-02T02:21:40.447344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈继延\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_word2vec_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:21:54.764830Z",
     "start_time": "2021-06-02T02:21:54.756851Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = lr.predict(X_word2vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T02:22:03.112120Z",
     "start_time": "2021-06-02T02:22:03.098211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8067"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = lr.score(X_word2vec_test,y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
