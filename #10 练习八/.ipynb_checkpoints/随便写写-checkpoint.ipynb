{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T08:30:41.877213Z",
     "start_time": "2021-05-14T08:30:41.859700Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import sys,os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "PATH = '../外部数据/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新词发现\n",
    "&emsp;&emsp;新词发现试图处理分词模型无法妥善处理的非通用语料：网络文档和领域文档。不同于分词，新词发现是一种无监督学习方法。  \n",
    "\n",
    "何谓“词”：  \n",
    "1. term在文章中出现的次数足够多 -> 频次足够大  \n",
    "2. 词的内部足够稳定 -> 互信息足够大  \n",
    "3. 左右邻字足够丰富 -> 左右邻字的熵足够大  \n",
    "\n",
    "**参考**：[新词发现](https://zhuanlan.zhihu.com/p/28095072)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T07:53:16.843808Z",
     "start_time": "2021-05-14T07:53:16.837257Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(PATH+'in_the_name_of_people.txt',encoding=\"utf-8\")\n",
    "text = f.read().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T07:54:38.782156Z",
     "start_time": "2021-05-14T07:54:38.777136Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '人民的名义',\n",
       " '\\n',\n",
       " '周梅森',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\u3000©中文在线数字出版集团股份有限公司，2016-2017',\n",
       " '\\n',\n",
       " '\\u3000数字版图书版权信息',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\u3000人民的名义/周梅森著.北京：中文在线数字出版集团股份有限公司，2017.2.',\n",
       " '\\n',\n",
       " '\\u3000CAEBN：7-001-000-60733638-6',\n",
       " '\\n',\n",
       " '\\u3000分类号：长篇小说',\n",
       " '——']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T08:40:05.674468Z",
     "start_time": "2021-05-14T08:40:05.654531Z"
    }
   },
   "outputs": [],
   "source": [
    "class sample_NewWordExtract():\n",
    "    def __init__(self, min_entropy=0.8, min_p=7, max_gram=4, min_freq=20):\n",
    "        self.min_entropy = min_entropy\n",
    "        self.min_p = min_p\n",
    "        self.max_gram = max_gram\n",
    "        self.min_freq = min_freq\n",
    "        self.all_terms = []\n",
    "    \n",
    "    # 去停用词\n",
    "    def drop_stopwords(self, text_lst, stopwords=False):\n",
    "        # 句间\n",
    "        punctuation = ['【','】',')','(','、','，','“','”','。','\\n','《','》',' ','-','！'\n",
    "                       ,'？','.','\\'','[',']','：','/','.','\"','\\u3000','’','．',',','…','?']\n",
    "        text_lst = [line for line in text_lst if line not in punctuation]\n",
    "        # 句内\n",
    "        if stopwords: # 还是不要去除停用词为好\n",
    "            with open(PATH+'stopwords.txt',encoding='utf-8') as stopwords:\n",
    "                for word in stopwords:\n",
    "                    upgrade = [text.replace(word.strip(), '') for text in text_lst]\n",
    "                    text_lst = [text for text in upgrade if text!='']\n",
    "        else: # 仅去除标点\n",
    "            for i in punctuation:\n",
    "                upgrade = [text.replace(i, '') for text in text_lst]\n",
    "                text_lst = [text for text in upgrade if text!='']\n",
    "                \n",
    "        return text_lst\n",
    "        \n",
    "    # 生成候选词\n",
    "    def creat_terms(self, text):\n",
    "        global all_terms\n",
    "        for gram in range(1, self.max_gram+1):\n",
    "            loop = len(text) - gram + 1\n",
    "            for i in range(loop):\n",
    "                self.all_terms.append(text[i:i+gram])\n",
    "    \n",
    "    # 信息熵\n",
    "    def entropy(): \n",
    "        pass\n",
    "    \n",
    "    # 互信息\n",
    "    def MI(): \n",
    "        pass\n",
    "    \n",
    "    # 新词提取\n",
    "    def extract(self,\n",
    "                text_lst, \n",
    "                stopwords=True,\n",
    "               ):\n",
    "        # 去停词\n",
    "        text_lst = self.drop_stopwords(text_lst)\n",
    "        # 划分候选词\n",
    "        for text in tqdm(text_lst):\n",
    "            self.creat_terms(text)\n",
    "        freq = FreqDist(self.all_terms)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T08:16:51.571721Z",
     "start_time": "2021-05-14T08:16:51.550770Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(PATH+'in_the_name_of_people.txt',encoding=\"utf-8\")\n",
    "text = f.read().split(\" \")\n",
    " \n",
    "stop_words = ['','\\ufeff','©','_','—','【','】',')','(','、','，','“','”','。','\\n','《','》',' ','-','！','？','.','\\'','[',']','：','/','.','\"','\\u3000','’','．',',','…','?']\n",
    "# 去停用词\n",
    "text_list = [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T08:40:12.625425Z",
     "start_time": "2021-05-14T08:40:11.883293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadd27150c7b4a9f9a0aeb02c11aa184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = sample_NewWordExtract()\n",
    "tmp = _.extract(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T08:40:14.118383Z",
     "start_time": "2021-05-14T08:40:14.063144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'的': 5823, '了': 5023, '一': 3709, '是': 3640, '不': 3489, '这': 2524, '我': 2237, '他': 2215, '在': 1943, '说': 1943, ...})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
