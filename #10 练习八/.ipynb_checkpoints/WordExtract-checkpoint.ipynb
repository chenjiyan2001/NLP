{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:28:45.134637Z",
     "start_time": "2021-05-13T11:28:45.083636Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import sys,os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "PATH = '../外部数据/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HanLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:00:30.561113Z",
     "start_time": "2021-05-13T02:54:10.565086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT\n",
      "[hone, QQ, iPad, HTC, 新浪微博, 平板电脑, 移动互联, 有机会, UI, ipad, MIUI, 原文评论, 原文转发, App, 我正在, 给力, 青年用, PHP, book, pple, CSDN, 条微博, 智能电视, Mac, Face, 关注小米, Q币, 即有机会, ··, 位好友, PAD, 官方微博, 转发此, 云技术, qq, 本条微博, CDMA, Web, Siri, 分享视频, 请关注, WIFI, 能助手, 有机会赢, QQ空间, 天翼QQ, 酷盘, ava, 开放购买, 开发工, 念乔布斯, 下载地址, IPAD, 给大家, 社交网络, 内人士, 新浪科技, ouch, 唯冠, CPU, SIM, Jobs, 伴随着, 索爱, 完美越狱, 新品发布, ···, okia, 电脑管家, 入网许可, 正式发布, 千元, 外媒, 品发布会, ack, 尤其是, 修的过来, 最新版本, 告诉我, 半年, SIM卡, 第四, 十二, 每天都, 消息称, 魅族MX, 随机抽取, USB, 什么呢, 移动终端, 小时内, 这么多, QQ秀, wifi, hink, 淘宝商城, 用户来说, hold, 已经被, 分享照片]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "人文艺术\n",
      "[ur, 术馆, in, th, 件事, he, an, ar, 第三, 做了个, 掌握在, 在这里, ing, 三十三, 背景中, 只动物, 过程中, 吕澎, ry, 洪凌, 本周, 总会有, 两种, 国古代, ve, yo, 王小, very, 街旁, 台灣, 雕刻时光, 有好心态, 中国当代, 宝钢员工, 子才可以, 新絲路, ow, 三届, 每个人都, 承泰, 天下之, 知道这些, 改变不了, 份工作, 這張, 都有一个, 代美术馆, us, 表现出, 于传统, 粗暴地, 付之成林, 给你一个, Art, 笑而不语, 第三届, 设计大赛, 另一个, 没有任何, 琴演奏, 柯岩, 欢迎大家, 痛而不言, 中央美, 收租院, 暂停时间, 换位思考, 利用价值, 利和自, 真正富有, 我们追, 张照片, 就叫, 当代摄影, 或组织, 客拍, 小勺子, 奇异景象, 定目标, 小鲁, 或负责, 中兴南路, ki, her, 你不知道, 周六下午, 体会到, 会这么, own, 原副, rm, is, 会有更, 会变得, 万瑞祥, 伊朗国家, ion, 件翡翠, 件东西, 们提供]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "传媒\n",
      "[新浪微博, 请关注, 敬请关注, 之声, 西卫视, 欢迎大家, 有机会, 官方微博, 江西卫视, 让我们, 西交通, 第三, 本周六, 请期待, 在线收听, 敬请收看, 浙江卫视, 金牌调解, 第一财经, 城市之, QQ, 中国之声, 贵州卫视, ww, 中央人民, 星光大道, 正在直播, 天晚上, 品聚, 周一至周, 城市之音, 上春晚, 海峡卫视, oo, 安徽卫视, CC, 湖北卫视, 播出时间, 周一到, 点半, 第七, 位幸运, 今天晚上, 旅游卫视, 广电总, 陪你, vi, 踊跃参与, DCG, 还可以, 至周六, 读者调查, 天津卫视, 文艺广播, 优优宝贝, 黄金时, 主流媒体, ea, 即将开, 金陵晚报, 南国都市, 道怎么, 锡都热线, 您一起, ay, 卖难, wi, 年以来, 开始啦, 这个活动, 谢谢大家, sina, 第三届, 非诚勿扰, 工作经验, 实名注册, 就觉得, 宁夏卫视, 汽车音乐, 百万, 大家继续, 正式开, 本周六晚, 欢迎收看, ok, 东方卫视, 歌声传奇, 江苏综艺, 第一次, 在哪里, 在微博上, 有关部门, 有奖互动, 提高了, 可直接, 可获得, 今夜有, 只要你, 希望大家, 风热线]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "体育\n",
      "[BA, NBA, CC, 湖南经视, CBA, 本周, 五星体育, 热刺, 国家德比, 足球之夜, 最佳教练, 场均, ba, 如果你, 官方微博, vs, 陕西人和, as, 广大球迷, 能成为, 级联赛, 直播预告, 邵佳一, 反赌, 原文转发, ing, 历史上, 第七, 给力, FIFA, 辽宁女篮, 本周末, 第二届, 以场均, 让我们, 蒂加纳, 在江西, 第一次, 上官鹏飞, 五套, 史上最, 为您带来, 高于中国, 天凌晨, all, 主场迎, 队主教练, WC, go, 那么多, 斯堡, ma, 大家一起, 姚明同学, AC米兰, 同时也, 童鞋们, 您认为, 指代表, 年度最佳, 视频直播, 皇马巴萨, 足球先生, 加盟申花, 将为您, ine, 杯篮球赛, 正式加盟, DP, 天晚上, 木有, 有多少, 阿尔滨, 你心目中, 明天凌晨, ww, 反赌扫黑, 州恒大, 竞赛频道, 主场迎战, 冬季转会, 李艳凤, 储物格, 正式宣布, 有奖竞猜, 青岛中能, 中奖观众, 世纪大战, 这里有, 在江西省, 过亿, 合同为期, 即将开, 称其, WCBA, 最佳球员, ig, 足坛反赌, 谢大家, 佛罗]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "健康\n",
      "[一定要, 抗衰, 是一种, 一份, 秋冬, 让你, 请收听, 季养生, 官方微博, 血管疾病, 预防感冒, 第二天, 研究人员, 排出体, 冬季养生, 请关注, 教你, 眼睛疲劳, 生姜红糖, 戴隐形, 补充维, 研究发现, 维生素C, 小智慧, 含丰富, 很容易, 效果更, 牙齿敏感, 着电脑, 会加重, 用脑过度, 色蔬, 爱康国宾, 生活方式, 手拍, 咽喉疼, 表现为, 几次, 温馨提示, H值, 滋阴润, 含量高, 第四, 富含维, 半小时, 可能引, 长痘, 大量排出, 市儿, 膳食纤维, 心肌梗, 锅内, 最佳时间, 注意保暖, 黄颖博士, 友们, 小百科, 瘦身减肥, 登特口腔, 不要太, 保护眼睛, 容易疲, 丰富的维, 七种, 肿瘤放, 第六, 一捏盐, 维生素A, 向一侧, 黑巧克力, 水浸泡, 养生保健, 含多种, 更有利于, 云南白, 过度用脑, 引起腹, 辛辣食, 导致肥胖, 之王, 易形成, 千万, 保护嗓子, 天就好, 族维生素, 多梦, 喝咖啡, 唇干, 接种疫苗, 阴润肺, 清热解, 咽干, 手脚冰, 减肥食品, 件事, 戴口罩, 肿瘤放疗, 绿叶蔬菜, 维生素E, 油腻食物]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "动漫\n",
      "[ay, 金龙奖, 陈维东, COS, com, 将于, 期精彩, 届中国, 十二, 轻小说, 猫老, QQ, 爱心龟, ww, ulu, 第二期, 银都文化, 二十, 第四届, 错过了, 奖公示, 痛车, 们一起, 南派三叔, 请关注, 欢迎大家, 在线观看, 票地址, 阿狸, 地球环保, 史上最, DOTA, ··, 官方微博, 第二届, ER, 希望能, 我们一起, 京电, 角川, 麦圈, 分享照片, 夏达, 给力, 文化艺术, 英雄联盟, 转发微博, 张小盒, 敢达, 让我们, 天闻角川, 为主题, wor, 名漫画家, 且还, 获奥斯卡, 衍生产品, 即将开, 发生了, 继续努力, 给大家, NAR, 节暨, 第五, 倒鸭子, ···, 作人员, 两位, 月新番, 一起玩, 开始连载, 千万, 而不是, 一定要, 十九, ACC, 秀大赛, 神秘人物, 「轻音」, 大家踊跃, 「穷神」, 力支持, 奖名单, 果揭, 电视系列, 为主角, 嘉年华暨, 国内首, 男主, 日上午, 智趣, 哆啦A梦, 最棒, 光明之, wer, 文化传播, 番「, 业发展, 且还是, QQ群]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "女性\n",
      "[请收听, 秋冬, 十二, 教你, 整形美容, 就收听, ww, 脸部保养, 第一名, 抗衰, 敷面膜, 十二星座, ng, 有机会, BB霜, 去黑, 准妈, 去痘, 注射除皱, om, 小技巧, 很简单, 美容医院, 一定要, 型师, 神仙水, 温馨提示, 收听→, 韩式, 种妙, 孕妈妈, ing, MM, 在脸上, ER, 官方微博, 秋冬季节, 小编, 减肥方法, 蚕丝面膜, 膜法世家, io, 来参加, 种妙方, 十分钟, ad, 让你从, 件事, me, RM, 徐州美莱, 教你美容, 很容易, do, 排毒养颜, 整形专家, 油脂分泌, 推荐给, 用温水, 清水洗, 婚纱摄影, 装新款, 伊博士, 缩毛孔, 植村秀, 美妆达人, 容器里, 时尚新娘, Di, 祛痘印, 千万, 怎么化, 减肥食品, 片敷, ve, 就有机会, 下蹲, XX, 补充维, 很重要, 草茶, 赶紧收, 整形医, 格纹, 含维生素, 给大家, 一点点, is, 更显, 越吃越, 收缩毛孔, 种切口, 丰胸手术, di, 秀蔓, 根香蕉, 想知道, 排出体, 筒靴, 用洗面奶]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "娱乐\n",
      "[★★, ★★★, 那些年, 龙门飞甲, 我们一起, 原文评论, 原文转发, 跨年, 亲密敌人, 有机会, ★★★★, 将于, 南卫视, ve, 官方微博, 就有机会, 江苏卫视, 吉思光, QQ, 五月天, 给力, 音乐广播, 卫视跨年, 这首歌, ing, 请关注, 第一次, 步步惊心, 抢票, 霆锋, 达人秀, 裸婚时代, 位好友, 浙江卫视, 袭警抢枪, ··, 组图, 盛乡, 安徽卫视, dy, by, QQ音乐, 非诚勿扰, 一定要, 林志, 希望大家, ove, 两位, igh, 敬请关注, 西卫视, 第四, 全球首, ww, ···, 时尚先生, one, 本周六, 筷子兄弟, 请期待, 美人天下, and, 独家首, 深海之战, 故事广播, 千万, 自土豆网, 东方卫视, 史上最, DJ, eli, 多部电视, 自优酷网, hone, 极速天使, 每个人, 摇滚春晚, 深圳卫视, 国际影城, 时收看, 有木有, 中饰演, 奖名单, 中国蓝, min, 朱大可, 孙菲菲, 大家多多, 不要错过, 真的很, 段林希, 更多精彩, 感谢大家, 乐金榜, ····, 最佳男, 世界巡, ste, 青檬, ive]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "广告公共\n",
      "[om, 环境事件, 有机会, 世界上, ack, QQ, 黄伟老师, 川宁, ad, 易家湖, 外来物种, 自优酷网, 飞音宝, ect, 低碳生活, 龙王拳, 绿色风, 上濒, 客卖, day, 好礼, 优秀的人, 年画宝, 个孩子, ww, ida, 邀您, 请关注, 一定要, 官方微博, 平板音响, 有机会获, 小编, 就断不致, 大成建筑, 坚持到, 川宁茶, 年画宝宝, 愿牵, 务员, 最具, 也是一种, 染了, 欢乐迪, 报名参, 腾讯微, 腾讯公益, 绿色圣诞, 绿家园, 绿色账户, 笑抽了, 居泰隆, 第一个, 不删档, 宽阔的是, ber, 精英测试, 这一天, 宝宝宝, 如果你, 好的决, 好乐买, 介绍一下, 央视广告, 天早上, 人的心量, 不仅仅是, 天下通, 人湖和谐, 五周岁, 在这一天, 狼隐契丹, 在江湖, 和预, 沙棘树, 周年庆, 二线城市, 了美丽, SH, 后裤, 桃源店三, 台庆, 只要我们, 发起了一, 个老乡, 春天百货, 卡酷岛, 新星组, 数量有限, 南昌晚报, 拉式, 报名参加, 欢迎大家, 正式开始, 护湖泊, 活动开始, 湘潭公, 然後, 千愿牵手, 千万]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "房产\n",
      "[线城市, 首套, 笋盘, 三线城市, 首套房贷, 日笋盘, 限购政策, 二三, 新浪房产, 调控政策, 原文转发, 商品住, 楼市限购, 天府新区, 环比下降, 墅质, 经济工作, 贷款利率, 多家银行, 部分银行, 将继续, 买房送, 三盛, 半岛阳光, 今日笋盘, 降价潮, 合理回, 销售金额, 西安楼市, 重点城市, 地方政府, ww, 有机会, 优惠利率, CPI, 住房保障, 康发, 明年经济, 销售面积, 调控目标, 老业主, 利率普遍, 过快, 也开始, 年同期, 补差价, 依诺维绅, 你猜, 沪深, 住宅用地, 比涨幅, 等多家, nd, 提前还, 最高优惠, 官方微博, 把房地产, 集中在, 销售业绩, 户型方正, 报名参, 陈晟, 豪装, 访时, 让你, 配套完善, 翡丽, 元左右, 胡大维, 小区环境, 中标价, 湾国际, 在接受, 消息传出, 住房建设, 看房团, 你猜你, 湖花园, 标杆房企, 次活动, om, 品牌中介, 武汉楼市, 银行首套, 价格继续, 是否会, 景湖花园, 他们对, 最具, 亿人民币, 提前还贷, 新浪乐居, 交通便, 五条, 互动话题, 二手房成, 报名参加, 二十余, 十大城市, GDP]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "教育\n",
      "[首羡, 江苏丰县, 校车侧翻, 四六级, 丰县首羡, 死亡人数, 发生重大, 发生侧, 日下午, 都市晨报, 英语四, 翻入河, 级考试, 徐州丰县, MBA, 发生一起, 江苏徐州, 自主招生, 教育盛典, 后屯, 发生侧翻, 最具, 意见稿, 官方微博, 负全责, 当事司机, 澳际, 日上午, 此次事, 张后屯村, 英孚, 县公安, 万通汽修, 阳市, 事故原因, 最具品牌, 送小学生, ng, 洪旭, 环球雅思, 广播电, 有关部门, 第一财经, 丰县公安, 省教育厅, 巨人学校, 试成绩, 自主选拔, 请关注, ia, 事故遇难, 中公教育, 公交专用, 交通违法, 报名参加, 生和家长, 业资格, er, 校车被撞, 几万, 全球最, 七十, 一定要, 被刑拘, 第四届, 跨考, 十二, 隆重举行, 给大家, 网上报名, 年自考, 安全知识, 站着, 严重交通, 严重超载, 工作经验, 道行驶, 这么多, 被挤, CC, 乡村教师, 考研政治, is, 优先通行, 因躲避, 新华电脑, 童鞋们, 亿美, si, re, EC, 届全国, 交巡, 欢迎大家, 时尚先生, 全球最大, 入路边, 之声, 乘车安全, 易贝乐]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "文学出版\n",
      "[意林, ★★, 意林杂志, ★★★, 学出版社, ★★★★, 篇小说, 双刊, 火花网, 漫友, 条微博, 位好友, 名车志, 沃杂志, 谢大家, er, 新书预, oo, 还没有, 乔布斯传, 官方微博, 轻小说, 请关注, 第四, 十二, 万字, 价比, 有机会获, 高尔泰, 萨冈的短, 二十, 百万, 恩施晚报, 十七, 时装男士, COS, 盗墓笔记, 哪些好书, 传媒集团, 尚舞, 你的心, 位朋友, SE, 再联想, 伴随着, 科莫多, 目击了, 淘宝店, 临沂广, 韶辉, 不要错过, 全国首, 时尚旅游, 尔夫, HE, FE, 您的支持, 扩为, 全球华, 全球首, 恩道尔, 谢谢大家, 请各位, 年度最佳, 角川, 希望大家, 这个世界, 小张对开, cos, 稚慧坊, 第二届, 这些作品, 在香港, 皓宸, 国际期刊, 去发现, 期沃杂志, 杂志铺, 且还, 史上最, 云中书城, tore, 看到这么, qq, Sa, 旅游杂志, 晒杂志, 千万, SM, 半月幽默, 投稿邮箱, 新书预告, 更重要, 所有参与, 感谢读者, 卢嵘, 内统一, 十周年, 把自己, 依因缘]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "旅游\n",
      "[原文转发, 原文评论, 有机会, 省旅游局, 友们, 室外滑雪, 国西部, 官方微博, 去哪, 李代山, 中国国家, 唯一一个, 请关注, 世界上最, 中国最美, 生活方式, 阿得莱德, 等什么, QQ, 嗨顺风, 届中国, ad, 上野公园, 最美街景, 景区免票, ng, 中国公民, 红叶节, 能看到, 为主题, 第一座, 个小时, 响沙, 滇藏, 正式开, 平遥古城, 老码头, ma, 想去哪, 最适合, 有机会赢, 新疆篇, 快来参, 七彩云南, 族风情, 优家画报, 被称, 详情登, 日本九州, 免票活动, 分享图片, 桦榭, 历史文化, 而又, 给力, 能欣赏到, 中国十大, 圣寺, 大家介绍, 嗨套餐, ww, 两日自, di, dd, 组织全, 畲族风情, 段时间, 球时尚, 能错过, 你敢, 本周日, 李栓科, 不要错过, 同行客, 杜甫草堂, 不容错过, Pa, 是否能, ing, 三峡红叶, 星酒店, 最接近, 构成了, 报名吧, 报名参加, 担保交易, 协兴, ia, 抢游惠, 快来参加, 快来报名, 古镇位于, 广州长隆, 专家学, 当地政, 尼乐园, 冰雪世界, iP, 安徽黄山, 世界美景]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "时尚\n",
      "[原文评论, 原文转发, 芭莎, 秋冬, 麦包, 麦包包, 沙宣, 卡诗, 婚纱摄影, vi, 东田造型, 本周, 让我们, 给力, 韶辉, 巴黎卡诗, 有机会, 莉婕, NE, 施华, IE, 如果你, 梦芭莎, 银泰, 高级定制, AM, 嘉琦, iv, 传媒集团, 也可以, ww, 幸福王国, 级发型师, 么样, 世纪佳人, 请点击, 官方微博, 请关注, IC, 第三, Cha, HA, 有机会获, 可享受, MI, 一定要, IN, 光美容, 折优惠, ion, ing, 米娜, 夏系列, 给大家, 排扣, 千万, 第六, 美丽说, 梓琳, shi, 广西卫视, 银泰中心, 第一次, 颁奖盛典, 等你来, EN, IR, 底液, DE, 潮饰女王, 不要错过, 婚纱礼服, 说出你, RA, 暖又, 施华蔻, 唐毅, ER, 半年, 别册, 抗衰, mm, 广州曙光, 等什么, 小清新, 欢迎关注, 谁说, 姚戈, 芭莎珠宝, 大抽奖, 国超模, 普罗蜜思, 新的一年, ers, 广场店, ell, 中国北部, 很简单, 打底毛衣, by]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "校园\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[将于, 高校环保, 绿色风, 技术学院, 环保社, ww, 环保协会, 第六, de, 环球资源, 业大学招, 创业咖啡, 术类, 安全教育, 网上报名, 误区是, 语演, SAT, om, 不知道, 个误区, 第十, 第四, 了一场, 州校友会, 一步, 考试时间, em, 双选, 连三, 颂钵, 艺考之家, 记忆大师, 车翻, 区决, 让孩子, 个误区是, 辽宁地区, 级考试, 第一位, 先生为, 淡水豚, 专业方向, 世界记忆, 公开招聘, 日举办, 届全国, 两所, 在这里, 所乡村, 技能展示, 徐小平, 南邮, 音乐类, 这句, 为目标, 十五, 之星, 请您, 动启动, 老师说, 舞蹈类, 决定于, 第二届, 已经开始, 第三届, 第五, 科技创新, 就非常, A志愿, 将于明, 画等, 第一讲, 绿色风将, 宣传活, 机构评, 术高考, har, 有明星脸, 天下午, 们带来, 本周日, 环保公益, 报名方式, 人担, 招生推, 些富豪, 教师招聘, 三十, 应试技巧, 启动仪式, 新疆文化, 编导类, 各类竞赛, 可登录, 可以免, 了关于, 去体验, 南二广场, 协会承办]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "汽车\n",
      "[原文转发, 原文评论, 州车展, 年度车型, 广州车展, 百事达, 国际车展, 有机会, 混合动力, 自主品牌, 科迈罗, 北京现代, 名车志, iP, 上海车展, 第七, BMW, CE, 请关注, 舒适性, 给力, 纯电动, 荣耀十年, SU, 广汽丰田, 日下午, 十二, 东风本田, ve, 召回部分, 涡轮增压, 千万, 最后一, 长安福特, 等什么, 正式发布, 海国际, 览会, 设计套装, 如果你, ran, 要注意, 奥拉驾, 届广州, 风挡玻璃, Po, 新迈腾, 性能版, 有哪些, 量同比, 学车费, ab, 东京车展, bri, 就有机会, 福特野马, 四门, IP, 细分市场, 本条微博, 汽车导报, 时尚座驾, 力更强, 万美元, 长沙车展, 座驾李, ay, 长城哈弗, 速仅, 超跑, Gran, 销售网络, 膜施工, 销量增速, 仅需, 管所, MINI, 精美车模, 双反, ive, 即可获, ig, 看到这, 段时间, 欧朗, 正式上市, 前脸, 万左右, 么样, II, 之星, 文化之旅, 最大功率, 微博直播, 共同开, 性方面, 为了保, yb, 中心举办, Bar]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "游戏\n",
      "[十大, 英雄联盟, 最受, QQ农, 魔兽世界, WCG, DNF, ame, 有机会, ww, 穿越火线, 不删档, 大冲锋, Q币, 官方微博, 给力, 分享视频, RPG, one, game, 网游公会, 最受期待, QQ农场, 电竞, FPS, 梦幻海底, DOTA, 风暴战区, 御龙, 让我们, 来看看, 请关注, 请大家, 快来领, 下载地址, 宣传视频, QQ音, QQ会员, 东珈, 就有机会, QQ飞车, Zv, web, 开放测试, 每天都, iPad, GDC, 年度最, Game, 并获得, 木有, 正式开启, 魏传, 精英测试, 来和我, 三国塔防, 还没有, 赶快来, 等你来, ook, ipad, 有木有, 之翼, Sky, 不知道, QQ秀, 望大家, WOW, 御龙在天, 等什么, 炫斗, CDEC, 将于今, 转发此, 仙境幻想, 经济系统, 来试试, 免费领, Pla, 三国无双, 真·, 今日正式, 问道外传, 和我一起, 快来抢, 家带来, 上古世纪, 推荐给, 条微博, G联赛, 准备好了, 广大玩家, 盛世三国, ouch, 妙妙, 史上最, 百万, 梦幻昆仑, 有机会得, 战争策略]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "生活\n",
      "[疯抢, om, ao, 有机会, ww, iP, 一定要, ba, ad, 金券, 官方微博, 给力, hone, QQ, 让我们, Pa, 千万, 就有机会, 预售期, 首羡, 电话订票, 最高气温, iPad, 童鞋们, 全国包邮, 好乐买, 警方提, 年春运, ve, 腾讯微, 教你, 呷哺, IP, 第四, 晒单, 正式开, 栖巢, 部分地区, 网络售票, 票预售期, 二十, 来看看, 温馨提示, 即有机会, 伊卡璐, 秋冬, 十二, 淘宝商城, 快来参, 绝味, 微卖场, 生活服务, 千万不要, 肉价格, 很不错, ke, 市气象, 希望大家, 我们一起, 让孩子, 工作经验, 之烧, 半小时, 欢迎大家, PA, 就像, 活动详情, 起来吧, 尊享, 被检, 境证件, 腾讯微博, 即将开, 请广大, 字头列车, 段时间, 用温水, MI, 送小米, 元话费, 请收听, 豪礼, 局出入境, 晨最低, 不住了, 绿瘦, 过程中, 即可获, 今天白天, Q币, 见图, 最高温, DIY, 大奖等, 喝→, 喝酸奶, 明天白天, 等着你, 关部门, 丰县首羡]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "美食\n",
      "[香天下, 栖巢, 有机会, 人套餐, ··, ia, vi, 洗干净, 豪享来, 撒上, 呷哺, 一定要, 沥干, DIY, 味多美, 拌均匀, 有机会获, 洗净切, 电饭, 搅拌均匀, 比比客, 给大家, 盛大开业, 小编, 分钟即可, 折优惠, 派悦坊, 时时乐, 不一样, 匀即可, QQ, 个小时, 美食美客, 美食情报, 狮诚, 很简单, 位好友, 秘制, 吉野家, DJ, 双人套餐, 吃货们, 韩式, ···, 沙县小吃, 锅烧热, 成泥, 有木有, 三样菜, 微茶楼, 就有机会, 教你, 吃遍赣州, 板长寿司, 这道菜, 最适合, 隆重开业, 请关注, 拌匀即可, ng, 粉勾芡, 沥干水, 一人一锅, 鸡蛋打, 破店, 呷哺呷哺, 味千拉面, 煎至, 锅魁, 松本楼, 如果你, 金券, 谢谢你, 大家推荐, 嫩滑, 国瑞, 上岛咖啡, 即可出锅, 你知道, 包旺, 订餐电话, 狮诚记, 尊享, 宫会, 梦幻餐厅, 过程中, 不合格, 免费试吃, 单人套餐, 圣诞大餐, 半价优惠, 洋葱餐厅, 给您, 三位好友, 活动详, 西新东方, 味道都不, 味食尚, 栖巢咖啡, 干水分]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "育儿\n",
      "[智灵通, 优优宝贝, 育儿贴士, BB, 进口奶粉, 内不要, 件事, 和睦家, 出生后, 看电视, 金宝贝, 在婴儿时, 绘本, 岁半, 在床上, 配方奶, 迪巧, 令营, 岁以下, 音乐胎教, 类食物, 让宝宝更, 很容易, 孕婴童, 点此, 智力发育, 易引发, 早教中心, 金色湖畔, 配方奶粉, 件事情, 他失去, 年画宝贝, 过程中, 该怎, 三岁以, 对胎儿, 三位, 千万不, 妈咪们, 添加辅食, 去医院, 有什么好, 稚慧坊, 种肉, 几本, 父母必读, 力发展, 一份, 容易引, 们赶紧, 得很好, 进行一次, 不宜多吃, 这个定位, 成长故事, 这几本, 他们的心, 岁左右, 人认为, 该怎么, 给婴儿吃, 节课, 容易引发, 二十, 让他失去, 躺在床上, 碎菜, 生活经验, 冠军宝贝, 内保持, 在线答疑, 室内光线, 爱的爸爸, 之星, 一直没有, 儿不宜吃, 家长必须, 和孩子共, 来报名吧, 同时关注, 两颗, 淘乐思, 爱上书本, 易消化, 协调能力, 星级考, 时间观念, 易导致, 各位妈妈, 千万不要, 有抽奖, 睡眠质量, 专家在线, 找妈妈, 不要错过, 护理专家, 三位好友, 动手操作, 斗妈大]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "财经\n",
      "[欧债, 早盘, 沪指, CPI, 欧债危机, 黄金白银, 监会, 经济工作, 个交易日, 第三, GDP, 沪深, 债务危机, 逢低, 品价格, 下调存, 同比增, K线, 股指期货, 欧美股市, 存准, ww, 调存款, 创新低, 逢高, 大宗商品, 重庆啤酒, 欧盟峰会, 低吸, 下调存款, 新华保险, 三季, 十二, 兑美元, PMI, 新浪财经, 四季度, 存准率, 经济增速, ng, 主权债, 净流, 背景下, 原文评论, 原文转发, 开低走, 预计今日, 十二五, QFII, 实体经济, 主权债务, 二级市场, 伦铜, 现货黄金, 千万, ME, 见底, 偏弱, 幅居前, 沪铜, 会主席, 天通金, 耐心等待, 连续第, 评级机构, 价值投资, 国际金价, 沪深两市, 触及跌停, 再创新低, 外汇占款, 小时图, 欧债问题, 年同期, 即期汇, 可可黄金, 在线解, 超跌反弹, 经济前景, 偏空, 冲高回落, 不排除, 基金经理, 地方政府, ETF, 牛证, 重点关注, 款利率, 幅下挫, 经济复苏, 净流出, QQ, 等收入, 超卖, 综合指数, 整数关口, GDP增, 金投网, IMF, 主力资金]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "WEIBO_PATH = \"weibo-classification/\"\n",
    "\n",
    "def test_weibo():\n",
    "    for folder in os.listdir(WEIBO_PATH):\n",
    "        print(folder)\n",
    "        big_text = \"\"\n",
    "        for file in os.listdir(os.path.join(WEIBO_PATH, folder)):\n",
    "            with open(os.path.join(WEIBO_PATH, folder, file), encoding=\"UTF-8\") as src:\n",
    "                big_text += \"\".join(src.readlines())\n",
    "        word_info_list = HanLP.extractWords(big_text, 100, True)\n",
    "        print(word_info_list)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "test_weibo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现成代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:00:51.229829Z",
     "start_time": "2021-05-13T03:00:43.201617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['郑西坡', '必须', '仿佛', '钟小艾', '汽油', '犯罪', '甚至', '孙连城', '包括', '领导', '九一六', '王大路', '易学习', '具体', '张照片', '喜欢', '常小虎', '渐渐', '结束', '湖畔', '孤鹰岭', '美食城', '共产党', '陈清泉', '反贪总局', '危险', '肖钢玉', '历史', '世界', '尤其是', '梁璐', '张华华', '省检察院', '组织', '准备', '或者', '努力', '老百姓', '材料', '研究', '拆迁', '赵立春', '掌握', '队伍', '希望', '油气集团', '参加', '秀秀', '矛盾', '奸商', '考虑', '刘庆祝', '估计', '组织部', '山水集团', '脑袋', '吴慧芬', '城市银行', '录音', '怀疑', '丈夫', '群众', '市公安局', '屏幕', '的政治', '麻烦', '熟悉', '谨慎', '百姓', '腐败', '贷款', '香港', '赵德汉', '尤会计', '的声音', '办公室', '朋友', '缓缓', '继续', '严肃', '王文革', '吴彩霞', '企业', '审讯', '招呼', '告诉', '赵瑞龙', '线索', '银行卡', '涉嫌', '原则', '杜伯仲', '迟疑', '田国富']\n"
     ]
    }
   ],
   "source": [
    "f = open(PATH+\"in_the_name_of_people.txt\",encoding=\"utf-8\")\n",
    "text = f.read()\n",
    "\n",
    "stop_word = ['【','】',')','(','、','，','“','”','。','\\n','《','》',' ','-','！'\n",
    "             ,'？','.','\\'','[',']','：','/','.','\"','\\u3000','’','．',',','…','?']\n",
    "for i in stop_word:\n",
    "    text = text.replace(i,\"\")\n",
    "\n",
    "# 词语最低熵\n",
    "min_entropy = 0.8 \n",
    "# 词语最低互信息\n",
    "min_p = 7\n",
    "# 词语最长长度\n",
    "max_gram = 4\n",
    "# 词语最低频率\n",
    "min_freq = 20\n",
    "\n",
    "# 生成候选词\n",
    "def ngram(text,max_gram):\n",
    "    unigram = [i for i in text]\n",
    "    loop = len(unigram) + 1 - max_gram\n",
    "    ngram = []\n",
    "    for i in range(loop):\n",
    "        ngram.append(text[i:i + max_gram])\n",
    "    if max_gram == 1:\n",
    "        return unigram\n",
    "    else:\n",
    "        return ngram\n",
    "\n",
    "#计算词的概率\n",
    "def probability(word):\n",
    "    len_word = len(word)\n",
    "    total_count = len(word_all[len_word])\n",
    "    prob = freq_all[len_word][word]/total_count\n",
    "    return prob\n",
    "\n",
    "def entropy(alist):\n",
    "    f = FreqDist(alist)\n",
    "    # 计算熵\n",
    "    ent = (-1)*sum([i/len(alist)*math.log(i/len(alist)) for i in f.values()])\n",
    "#     ent = 0\n",
    "#     for value in f.values():\n",
    "#         p = value/len(alist)\n",
    "#         ent += -1*p*math.log(p)\n",
    "    return ent\n",
    " \n",
    "freq_all = [0]\n",
    "word_all = [0]\n",
    "for i in range(1,max_gram+1):\n",
    "    t = ngram(text,i)\n",
    "    freq = FreqDist(t)\n",
    "    word_all.append(t)\n",
    "    freq_all.append(freq)\n",
    "    \n",
    "#筛选一部分符合互信息的单词\n",
    "final_word=[]\n",
    "for i in range(2,max_gram+1):\n",
    "    for j in word_all[i]:\n",
    "        # 小于最小频率的直接过滤\n",
    "        if freq_all[i][j] < min_freq:\n",
    "            continue\n",
    "\n",
    "        # 两个字以上的词，看成是两部分构成的，计算所有可能的组合的互信息，取最小的作为整个词的互信息\n",
    "        p = min([probability(j[:k])*probability(j[k:]) for k in range(1,len(j))])\n",
    "        if math.log(probability(j)/p) > min_p:\n",
    "            final_word.append(j) \n",
    "\n",
    "            \n",
    "#         p_list = []\n",
    "#         for k in range(1,len(j)):\n",
    "#             #计算互信息\n",
    "#             p = probability(j[:k])*probability(j[k:])\n",
    "#             p_list.append(p)\n",
    "#         p_min = min(p_list)\n",
    "#         if math.log(probability(j)/p_min) > min_p:\n",
    "#             final_word.append(j)\n",
    "\n",
    "# 去重\n",
    "final_word=list(set(final_word))\n",
    "\n",
    "#筛选左右熵\n",
    "final_result=[]\n",
    "for word in final_word:\n",
    "    # 找出一个词前后出现的词\n",
    "    lr = re.findall('(.)%s(.)'%word,text)\n",
    "    left_entropy = entropy([w[0] for w in lr])\n",
    "    right_entropy = entropy([w[1] for w in lr])\n",
    "    # 取左右熵的最小值进行比较\n",
    "    if min([right_entropy,left_entropy]) > min_entropy:\n",
    "        final_result.append(word)\n",
    "        \n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:00:56.505198Z",
     "start_time": "2021-05-13T03:00:54.862148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[侯亮平, 达康, 李达康, 高育良, 祁同伟, 蔡成功, 沙瑞金, 高小琴, 丁义珍, 陆亦可, 欧阳菁, 季昌明, 委书记, 京州, 陈岩石, 刘新建, 郑西坡, 赵瑞龙, 赵东来, 大风厂, 易学习, 山水集团, H省, 赵立春, 二十, 王大路, 孙连城, 吴慧芬, 陈清泉, 肖钢玉, 尤会计, 光明湖, 吕州, 田国富, 刘庆祝, 吴彩霞, 赵德汉, 王文革, 九一六, 京州市, 钟小艾, 十万, 省检察院, 杜伯仲, 祁厅长, 四十, 千万, 十五, 张华华, 梁璐, 五十, 张树立, 油气集团, 百万, 十八, 常小虎, 张照片, 高小凤, 秦老师, 孤鹰岭, 岩台, 州市委, 反贪总局, 城市银行, 光明区, 群访, 看了看, 陆处长, 赵公子, 大风服装, 五千, 郑胜利, 站起来, 没办法, 秦局, 安置费, 政治资源, 张卡, 怔了一下, 田杏枝, GDP, H大学, 三张照片, 站了起来, 光明湖畔, 京州市委, 二〇, 二十五, 屏幕上, 想了想, 检察警车, 秦局长, 猴崽子, 十七, 吴春林, 人民群众, 摇了摇, 几句, 二十八, 二百万]\n"
     ]
    }
   ],
   "source": [
    "word_info_list = HanLP.extractWords(text, 100, True)\n",
    "print(word_info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T12:19:05.704011Z",
     "start_time": "2021-05-13T12:19:05.685019Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(PATH+'in_the_name_of_people.txt',encoding=\"utf-8\")\n",
    "text = f.read().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:39:56.426375Z",
     "start_time": "2021-05-13T11:39:56.420950Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_NewWordExtract(text, min_entropy=0.8, min_p=7, max_gram=4, min_freq=20):\n",
    "    # 过滤停用词\n",
    "    punctuation = ['【','】',')','(','、','，','“','”','。','\\n','《','》',' ','-','！'\n",
    "                   ,'？','.','\\'','[',']','：','/','.','\"','\\u3000','’','．',',','…','?']\n",
    "    text_list = [word for word in text if word not in punctuation]\n",
    "    with open(PATH+'stopwords.txt',encoding='utf-8') as stopwords:\n",
    "        for word in stopwords:\n",
    "            upgrade = [text.replace(word.strip(), '') for text in text_list]\n",
    "            text_list = [text for text in upgrade if text!='']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:01:10.807723Z",
     "start_time": "2021-05-13T03:01:10.798364Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    生成数据\n",
    "    \"\"\"\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    return postingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:01:15.023624Z",
     "start_time": "2021-05-13T03:01:15.014114Z"
    }
   },
   "outputs": [],
   "source": [
    "def tf_idf_calc(doc_list):\n",
    "    # 构建词典\n",
    "    word_dictionary = {}\n",
    "    for doc in doc_list:\n",
    "        for word in doc:\n",
    "            word_dictionary[word] = word_dictionary.setdefault(word,0) + 1\n",
    "\n",
    "    # 计算每个词在每个文档中的词频\n",
    "    word_tf_list = []\n",
    "    for doc in doc_list:\n",
    "        word_tf = {} \n",
    "        for word in doc:\n",
    "            word_tf[word] = word_tf.setdefault(word,0)+1/len(doc)#统计词频的同时进行归一化处理\n",
    "        word_tf_list.append(word_tf)\n",
    "    #print(word_tf_list)\n",
    "\n",
    "    #计算每个词的IDF值\n",
    "    doc_num = len(doc_list)\n",
    "    word_doc = {} #存储包含该词的文档数\n",
    "    word_idf = {} #存储每个词的idf值\n",
    "    for word in word_dictionary:\n",
    "        for doc in doc_list:\n",
    "            if word in doc:\n",
    "                word_doc[word] = word_doc.setdefault(word,0) + 1\n",
    "    #print(word_doc)\n",
    "    for word in word_dictionary:\n",
    "        word_idf[word] = math.log((doc_num+1)/(word_doc[word]+1))+1 # 进行平滑\n",
    "    #print(word_idf)\n",
    "\n",
    "    #计算每个词的TF*IDF的值\n",
    "    res_tf_idf = []\n",
    "    for i in range(len(doc_list)):\n",
    "        word_tf_idf = {}\n",
    "        for word in doc_list[i]:\n",
    "            word_tf_idf[word] = word_tf_list[i][word]*word_idf[word]\n",
    "        res_tf_idf.append(word_tf_idf)\n",
    " \n",
    "    return word_dictionary,res_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:01:18.028404Z",
     "start_time": "2021-05-13T03:01:18.018719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my 0.2228022554193461\n",
      "dog 0.2228022554193461\n",
      "has 0.32182328121362397\n",
      "flea 0.32182328121362397\n",
      "problems 0.32182328121362397\n",
      "help 0.32182328121362397\n",
      "please 0.32182328121362397\n"
     ]
    }
   ],
   "source": [
    "doc_list= loadDataSet() #加载数据\n",
    "dictionary,res = tf_idf_calc(doc_list) #每个词在不同文档中的TF-IDF值\n",
    "for word, tf_idf in res[0].items():\n",
    "    print(word,tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:01:32.326275Z",
     "start_time": "2021-05-13T03:01:32.317158Z"
    }
   },
   "outputs": [],
   "source": [
    "class UndirectWeightedGraph:\n",
    "    d = 0.85\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    "\n",
    "    def addEdge(self, start, end, weight):\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    "\n",
    "    def rank(self):\n",
    "        ws = defaultdict(float)\n",
    "        outSum = defaultdict(float)\n",
    "\n",
    "        # 权重的初始值\n",
    "        wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "        for n, out in self.graph.items():\n",
    "            ws[n] = wsdef # 权重初始化\n",
    "            outSum[n] = sum((e[2] for e in out), 0.0) # 计算权重的和\n",
    "\n",
    "        sorted_keys = sorted(self.graph.keys())\n",
    "        for x in range(10):  # 循环10次\n",
    "            for n in sorted_keys:\n",
    "                s = 0\n",
    "                for e in self.graph[n]:\n",
    "                    s += e[2] / outSum[e[1]] * ws[e[1]]\n",
    "                ws[n] = (1 - self.d) + self.d * s\n",
    "\n",
    "        (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])\n",
    "\n",
    "        for w in ws.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    "\n",
    "        for n, w in ws.items():\n",
    "            # 归一化\n",
    "            ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)\n",
    "\n",
    "        return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:01:34.664553Z",
     "start_time": "2021-05-13T03:01:34.646108Z"
    }
   },
   "outputs": [],
   "source": [
    "def textrank(sentence, topK=5):\n",
    "        \"\"\"\n",
    "        Extract keywords from sentence using TextRank algorithm.\n",
    "        Parameter:\n",
    "            - topK: return how many top keywords.\n",
    "        \"\"\"\n",
    "        span = 5\n",
    "        g = UndirectWeightedGraph()\n",
    "        cm = defaultdict(int)\n",
    "        words = jieba.lcut(sentence)\n",
    "        for i, word in enumerate(words):\n",
    "            for j in range(i + 1, i + span):\n",
    "                if j >= len(words):\n",
    "                    break\n",
    "                else:\n",
    "                    cm[(word, words[j])] += 1\n",
    "\n",
    "        for terms, w in cm.items():\n",
    "            g.addEdge(terms[0], terms[1], w)\n",
    "        nodes_rank = g.rank()\n",
    "        tags = sorted(nodes_rank, key=nodes_rank.__getitem__, reverse=True)\n",
    "\n",
    "        return tags[:topK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T03:01:38.022725Z",
     "start_time": "2021-05-13T03:01:37.169475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\陈继延\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.827 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['的', '中文', '更好', 'jieba', '比']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank(\"还有什么是比jieba更好的中文分词工具呢？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
